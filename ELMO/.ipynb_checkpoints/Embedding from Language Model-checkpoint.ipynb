{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding from Language Model(ELMO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word Embedding은 Natural Language Processing을 하는데 있어서 가장 기본적으로 해야하는 일이다.\n",
    "- 하지만 Word Embedding 기법에는 Word2Vec, FastText, Glove, ... 와 같은 방법들이 있다. 하지만 다음과 같은 방식은 문장의 문맥과는 상관없이 유사한 단어 끼리 모이도록 학습을 진행한다.\n",
    "- Deep contextualized word representations 논문에서는 Bi-directional LSTM을 사용해서 Word embedding을 하는 것이 좋은 결과를 낸다고 말한다.\n",
    "  - 그 이유는, 같은 단어가 항상 같은 값을 가지는 것이 아닌 문맥에 따라서 다른 값을 가지도록 학습하기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained Bi-directional LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 많은 양의 데이터를 이용해서 Bi-directional LSTM으로 학습을 진행한다.\n",
    "- 그 결과 문맥에 따라 다른 값을 가지는 Word Embedding을 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Bi-directional LSTM, Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 양방향 input을 사용할 뿐만 아니라 깊은 신경망을 사용한다.\n",
    "- Bi-directional LSTM에서 나온 결과에 weights를 주고 합하여 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-directional LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 수식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ p(t_1, t_2, ..., t_N) = \\prod_{k = 1}^N p(t_k\\;|\\; t_1, t_2, ..., t_{k-1}) $$<br>\n",
    "$$ p(t_1, t_2, ..., t_N) = \\prod_{k=1}^N p(t_k\\;|\\; t_{k + 1}, t_{k + 2}, ..., t_N $$<br>\n",
    "$$ \\sum_{k = 1}^N \\left( log p(t_k\\;|\\; t_1, ..., t_{k -1};\\theta_x, \\overset{\\rightarrow}{\\theta}_{LSTM}, \\theta_s) + log p(t_k\\;|\\; t_{k + 1}, ..., t_N; \\theta_x, \\overset{\\leftarrow}{\\theta}_{LSTM}, \\theta_s) \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Residual network에서 사용하는 shortcut 방식을 이용해서 Vanishing gradient 문제를 피할 뿐만 아니라,\n",
    "- Recurrent Nueral Network의 경우 장기기억 보존이 힘든데 그것을 보존해 준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ ELMo^{task}_k = E(R_k;\\theta^{task}) = \\gamma^{task} \\sum^L_{j=0} s^{task}_j h^{LM}_{k,j} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Pre-trained된 Bi-directional LSTM에 Embedding하려는 문장을 input으로 넣으면 embedding된 결과가 나옴.\n",
    "2. Bi-directional LSTM 각 layer의 vector 합을 각 layer 가중치를 적용해서 구함.\n",
    "3. LSTM sequence 길이 만큼 vector가 생성됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- softmax를 사용하는 것을 볼 수 있는데 argmax했을 때 나오는 값을 쓰는 것도 좋지만 layer 마다 가중치를 줬을 때 가장 좋은 결과였다고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[BERT 참조 블로그](http://hugrypiggykim.com/2018/06/08/elmo-deep-contextualized-word-representations/)<br>\n",
    "[BERT 논문](https://arxiv.org/pdf/1802.05365.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
