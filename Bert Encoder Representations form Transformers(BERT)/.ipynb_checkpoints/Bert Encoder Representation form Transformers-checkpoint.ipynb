{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Encoder Representation form Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pre-trained 함으로써 성능을 올릴 수 있도록 만든 Model이다.\n",
    "- Google에서 제시한 모델이며 아래 논문에서 참고하였다.<br><br>\n",
    "[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 많은 NLP 문제에서 Language model pre-training은 성능향상에 도움을 준다고 입증되었다.\n",
    "- ELMo, GPT와 같은 다양한 모델에서 pre-training을 해서 NLP 성능을 향상시켰다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pre-train\n",
    "  - feature-base\n",
    "    - ELMo, 사전에 학습한 feature를 사용하지만, 각각의 문제에 맞는 architecture를 추가하여 문제를 해결한다.\n",
    "    - 따라서, 사전에 학습을 통해서 language representation만 사용하고 model은 따로 만든다.\n",
    "  - fine-tuning\n",
    "    - GPT, 각각의 문제에 맞는 새로운 parameter를 사용한다. 그래서 이전에 학습한 parameter를 이용해서 fine-tuning하는 방법이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단방향으로 language model을 학습하기 때문에 성능 저하가 있다. 또한, 단방향이기 때문에 pre-train과정에서 사용될 architecture를 선택할 수 있는 폭이 줄어든다.\n",
    "- **따라서, fine-tuning based 방법으로 BERT를 사용할 예정이다. 앞서 ELMo, GPT는 단방향이었지만 현재 모델에서는 Masked Language Model(MLM)을 사용하여 양뱡향으로 학습시킬 것이다.**\n",
    "  - Masked Language Model, 임의로 input 단어 몇 개를 masking하고 다른 단어를 이용해서 masking된 단어를 예측하는 것으로 학습한다. 이것을 통해서 기존 연구의 경우 왼쪽에서 오른쪽으로 학습을 하였지만 하나의 문장에서 masking을 하기 때문에 양방향의 단어를 보고 masking된 단어를 추론하는 식으로 학습이 된다. 따라서 문맥이 적절하게 학습된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 추가적으로, Next sentence prediction을 사전 학습에 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 요약"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 양방향으로 학습하는 것은 매우 중요하다.\n",
    "- ELMo도 bi-directional LSTM을 사용하기 때문에 양방향이라고 할 수 있지만 Concatenate해서 사용한다는 것이 BERT와 다른점이다.<br><br>\n",
    "[google github code](https://github.com/google-research/bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![default](https://user-images.githubusercontent.com/22078438/51461268-f1c98700-1da0-11e9-835c-530bd9e4e2d1.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BERT는 multi-layer 양방향 transformer encoder 구조를 가지고 위 그림에서 볼 수 있듯이 각각의 모델을 비교해볼 수 있다.\n",
    "- annotation\n",
    "  - L, transformer의 layer수(transformer 블록수)\n",
    "  - H, hidden size\n",
    "  - A, attention head의 개수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 주어진 token(단어)에 대해서 아래 3가지 값을 더해서 input으로 사용한다.\n",
    "  - Token embedding, 각 token 마다 embedding이 따로 있음. Word2Vec과 비슷한 형식인 것 같음.\n",
    "  - Segment embedding, 각 문장에서 나온 sentence embedding을 다 더함.\n",
    "  - Position embedding, input이 무엇인지는 중요하지 않고 input의 위치가 중요하다. 어느위치에 있는지 알기 위해서 position embedding을 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![default](https://user-images.githubusercontent.com/22078438/51461679-09553f80-1da2-11e9-8278-21e972048c61.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 그림에서 볼 수 있듯이 모든 문장 첫 token에는 special classification embedding값(CLS)를 넣어준다. 각각의 token에 대응하는 마지막 hidden state는 classification 문제에서 사용된다. classification 문제가 아니라면 버린다.\n",
    "- 한 번에 두 개의 문장이 들어오면 special token(SEP)를 이용해서 구분해준다. 두 번째 방법은 학습된 문장 A embedding 값을 앞쪽 문장 모든 token에 더해주고 문장 B embedding 값을 뒤쪽 문장 모든 token에 더해줌."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-training Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 양방향으로 학습할 수 있게 하기 위해서 문장 중간에 masking을 해서 masking 부분을 예측하도록 학습하였다.\n",
    "- 하지만 위 방법을 사용하게 됨으로써 양방향 token간의 관계를 정확히 파악할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pre-training 시점에서는 [MASK] token을 사용하지만 fine-tuning한 후에는 사용하지 않기 떄문에 pre-training과 fine-tuning사이에 차이가 생긴다.\n",
    "  - 해결, 학습 과정에서 모든 masking된 값을 [MASK] token으로 바꾸지 않는다.\n",
    "  - 대부분의 데이터는 [MASK]로 대체를 하지만 몇몇 데이터는 임의의 다른 단어로 대체를 하고, 몇몇 데이터는 그대로 놔둔다.\n",
    "  - 위 방법을 사용하게 되면서 pre-train과 fine-tuning과의 격차가 줄어든다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Sentence Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 두 개의 문장이 input으로 들어 왔을 때, 두 문장이 이어지는 문장인지?(IsNext) 아닌지?(NotNext) 예측하는 모델이다.\n",
    "- Question Answering과 같은 문제들은 두 문장 사이의 관계를 찾는 것이 굉장히 중요하다. 따라서 문장 사이의 관계를 모델이 학습할 수 있도록 단일 Corpus로 구성된 두 개의 문장에 대해 문장이 관계가 있는지 없는지 binary classification하는 next sentence prediction 문제를 통해서 pre-training 한다.\n",
    "- 구체적으로 두 문장 A,B가 있을 때 학습 할 때 절반은 B를 실제 A의 다음 문장으로 선택하고, 나머지는 임의의 다른 문장을 선택한다. 다음 문장이 아닌 경우는 완전히 임의로 선택을 했다. 그 결과 pre-training 모델의 경우 97~98% 정확도를 보였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-training Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pre-training을 할 때 2개의 Corpus를 합쳐서 사용했다.\n",
    "- 각 학습 입력값을 만들기 위해서 두 문장을 뽑아서 하나의 문장으로 만든다.\n",
    "- 첫 번째 문장의 경우 A embedding 값을 가지고 두 번째 문장의 경우 B embedding 값을 가진다.\n",
    "- B의 50%는 실제 A의 다음 문장을 사용하고, 나머지는 임의의 문장을 사용한다.\n",
    "- 위 데이터를 이용해서 Next Sentence Prediction 문제에 사용한다.\n",
    "- 문장 최대 길이는 512 token으로 제한한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Classification 문제에서 입력값에 대해서 고정된 길이의 vector를 추출해야한다. 따라서 모델에서 첫 번째 입력값 [CLS] token에 의해 출력되는 마지막 hidden state 값을 사용한다.\n",
    "- fine-tuning 과정에서 전체 parameter는 그래도 유지된다 하지만 마지막 classification을 위해서 classification layer \\\\(W \\in \\mathcal{R}^{K \\times H} \\\\)를 추가한다. \\\\(K\\\\)는 classification 해야하는 정답 갯수이다.\n",
    "- 이후 softmax function을 취하여 확률 vector를 뽑는다.<br><br>\n",
    "$$ P = softmax(CW^T) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[BERT 참조 블로그](https://reniew.github.io/47/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
